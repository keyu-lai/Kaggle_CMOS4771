{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "52\n",
      "(1000, 52)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import classifier\n",
    "reload(classifier)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from classifier import encode_onehot\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def prepare_data(df):\n",
    "    df1 = pd.DataFrame(df.loc[:,[u'0',u'5',u'7',u'8',u'9',u'14',u'16',u'17',\n",
    "                          u'18',u'20',u'25',u'26',u'56']])\n",
    "    df2 = pd.DataFrame(df.loc[:,[u'23',u'57',u'58']])\n",
    "    df3 = pd.DataFrame(df.drop([u'0',u'5',u'7',u'8',u'9',u'14',u'16',u'17',\n",
    "                          u'18',u'20',u'23',u'25',u'26',u'56',u'57',u'58'],axis=1))\n",
    "    \n",
    "    return df1, df2, df3\n",
    "\n",
    "def encode(dataset):\n",
    "    drop_cols = []\n",
    "    for i in dataset.columns:\n",
    "        if isinstance(dataset[i].values[0], basestring):\n",
    "            drop_cols = drop_cols + [i]\n",
    "    \n",
    "    return encode_onehot(dataset, drop_cols)\n",
    "\n",
    "\n",
    "train = pd.DataFrame.sample(pd.read_csv('data.csv'),1000)\n",
    "label = train['label']\n",
    "train = train.drop('label',1)\n",
    "print len(train.columns)\n",
    "X_quiz = pd.read_csv('quiz.csv')\n",
    "\n",
    "#split the dataset\n",
    "train_1, train_2, train_3 = prepare_data(train)\n",
    "quiz_1, quiz_2, quiz_3 = prepare_data(X_quiz)\n",
    "                             \n",
    "\n",
    "#agg_data = pd.concat([train, X_quiz],axis=0,ignore_index=True)\n",
    "\n",
    "\n",
    "#display(agg_data.shape[0])\n",
    "#train = agg_data._slice(slice(0,train.shape[0]),0)\n",
    "#X_quiz = agg_data._slice(slice(train.shape[0],agg_data.shape[0]),0)\n",
    "#pca = PCA(n_components=100)\n",
    "#train = pca.fit(train).transform(train)\n",
    "#display(train.shape)\n",
    "#display(X_quiz.shape)\n",
    "\n",
    "print len(train_1.columns) + len(train_2.columns) + len(train_3.columns)\n",
    "print train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "from classifier import train_svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_3 = encode(train_3)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(train_3, label)\n",
    "\n",
    "#svm_clf = train_svm(X_train3, y_train3)\n",
    "svm_clf = SVC(kernel='poly', C=1.0, verbose=1).fit(X_train3, y_train3)\n",
    "predictions = svm_clf.predict(X_test3)\n",
    "print classification_report(y_test3, predictions)\n",
    "\n",
    "xtree = ExtraTreesClassifier(n_jobs=-1,n_estimators=200,max_depth=None, max_features='sqrt',random_state=0)\n",
    "xtree.fit(X_train3, y_train3)\n",
    "predictions = xtree.predict(X_test3)\n",
    "print classification_report(y_test3, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clf__average', 'clf__l1_ratio', 'clf__penalty', 'clf__warm_start', 'clf__random_state', 'clf', 'clf__n_iter', 'clf__learning_rate', 'clf__verbose', 'steps', 'clf__loss', 'clf__class_weight', 'clf__fit_intercept', 'clf__power_t', 'clf__n_jobs', 'clf__eta0', 'clf__alpha', 'clf__shuffle', 'clf__epsilon']\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "-- Epoch 1\n",
      "Norm: 23.02, NNZs: 33, Bias: 0.304084, T: 499, Avg. loss: 533.381897\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.81, NNZs: 33, Bias: -3.719816, T: 998, Avg. loss: 377.859071\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.63, NNZs: 33, Bias: -2.187557, T: 1497, Avg. loss: 299.342091\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.12, NNZs: 33, Bias: -0.819241, T: 1996, Avg. loss: 250.458674\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.35, NNZs: 33, Bias: -0.846817, T: 2495, Avg. loss: 216.775159\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 26.88, NNZs: 33, Bias: 0.846839, T: 500, Avg. loss: 540.865625\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.97, NNZs: 33, Bias: 0.740469, T: 1000, Avg. loss: 387.385494\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 24.69, NNZs: 33, Bias: 2.792540, T: 1500, Avg. loss: 308.386019\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 16.97, NNZs: 33, Bias: 1.808922, T: 2000, Avg. loss: 259.346941\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11.98, NNZs: 33, Bias: 1.339112, T: 2500, Avg. loss: 225.161174\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 22.89, NNZs: 31, Bias: -0.070369, T: 501, Avg. loss: 487.782520\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.67, NNZs: 31, Bias: 1.508519, T: 1002, Avg. loss: 351.323875\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.46, NNZs: 31, Bias: 1.318522, T: 1503, Avg. loss: 278.816314\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11.39, NNZs: 31, Bias: 0.860544, T: 2004, Avg. loss: 233.921081\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.35, NNZs: 31, Bias: 0.436168, T: 2505, Avg. loss: 202.772871\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.90, NNZs: 33, Bias: -0.028250, T: 499, Avg. loss: 116.536964\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.35, NNZs: 33, Bias: -0.092701, T: 998, Avg. loss: 71.175266\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.34, NNZs: 33, Bias: -0.134980, T: 1497, Avg. loss: 52.676746\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.10, NNZs: 33, Bias: -0.070841, T: 1996, Avg. loss: 42.327824\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.95, NNZs: 33, Bias: -0.038038, T: 2495, Avg. loss: 35.607442\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.63, NNZs: 33, Bias: -0.138793, T: 500, Avg. loss: 110.893957\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.08, NNZs: 33, Bias: -0.212900, T: 1000, Avg. loss: 69.600368\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.32, NNZs: 33, Bias: -0.097611, T: 1500, Avg. loss: 51.962776\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.26, NNZs: 33, Bias: -0.230210, T: 2000, Avg. loss: 41.971905\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.76, NNZs: 33, Bias: -0.152252, T: 2500, Avg. loss: 35.453884\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.06, NNZs: 31, Bias: 0.476496, T: 501, Avg. loss: 116.636635\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.19, NNZs: 31, Bias: 0.587823, T: 1002, Avg. loss: 70.546391\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.52, NNZs: 31, Bias: 0.448479, T: 1503, Avg. loss: 52.045175\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.13, NNZs: 31, Bias: 0.487663, T: 2004, Avg. loss: 41.722391\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.42, NNZs: 31, Bias: 0.492790, T: 2505, Avg. loss: 35.032694\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 16.44, NNZs: 32, Bias: 5.011831, T: 750, Avg. loss: 451.652656\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.59, NNZs: 32, Bias: 2.301155, T: 1500, Avg. loss: 304.544004\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.25, NNZs: 33, Bias: 3.000351, T: 2250, Avg. loss: 235.613425\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.37, NNZs: 33, Bias: 1.871957, T: 3000, Avg. loss: 194.494798\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11.81, NNZs: 33, Bias: 1.856283, T: 3750, Avg. loss: 166.831512\n",
      "Total training time: 0.00 seconds.\n",
      "Best score: 0.403\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.25      0.01      0.01       143\n",
      "          1       0.42      0.97      0.59       107\n",
      "\n",
      "avg / total       0.32      0.42      0.26       250\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "import classifier\n",
    "reload(classifier)\n",
    "\n",
    "from classifier import train_SGD_SVM\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svm_clf = train_SGD_SVM(X_train3, y_train3)\n",
    "predictions = svm_clf.predict(X_test3)\n",
    "print classification_report(y_test3, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.681\n",
      "Best parameters set:\n",
      "\tclf__n_estimators: 100\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.69      0.80      0.74       133\n",
      "          1       0.72      0.60      0.65       117\n",
      "\n",
      "avg / total       0.71      0.70      0.70       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import classifier\n",
    "reload(classifier)\n",
    "\n",
    "from classifier import train_ada_boost\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dt_clf = train_ada_boost(X_train3,y_train3)\n",
    "predictions = dt_clf.predict(X_test3)\n",
    "print classification_report(y_test3, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.69      0.77      0.73       139\n",
      "          1       0.66      0.56      0.60       111\n",
      "\n",
      "avg / total       0.67      0.68      0.67       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import classifier\n",
    "reload(classifier)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from classifier import train_xtree_classifer, train_rf, train_SGD_SVM, train_friedman_xgbm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# XTree is okay in set 3\n",
    "\n",
    "train_3 = encode(train_3)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(train_3, label)\n",
    "\n",
    "xtree = ExtraTreesClassifier(n_jobs=-1,n_estimators=200,max_depth=None, max_features='sqrt',random_state=0)\n",
    "xtree.fit(X_train3, y_train3)\n",
    "predictions = xtree.predict(X_test3)\n",
    "print classification_report(y_test3, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
