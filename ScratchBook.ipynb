{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "(95127, 52)\n",
      "52\n",
      "(126837, 52) (95127, 10) (95127, 6) (95127, 36)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import classifier\n",
    "reload(classifier)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from classifier import encode_onehot\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def prepare_data(df):\n",
    "    df4 = df\n",
    "    df1 = pd.DataFrame(df.loc[:,[u'0',u'5',u'7',u'8',u'9',u'14',u'16',u'17',u'56',u'57']])\n",
    "    #0 , 26 8, 5 , 14, 23 , 58 -> boss\n",
    "    #17, 56, 16,9,, 25, 57 -> yes\n",
    "    #20, 18, 7 -> no\n",
    "    #df2 = pd.DataFrame(df.loc[:,[u'18',u'20',u'23',u'25',u'26']])\n",
    "    df2 = pd.DataFrame(df.loc[:,[u'18',u'20',u'23',u'25',u'26',u'58']])\n",
    "    df3 = pd.DataFrame(df.drop([u'0',u'5',u'7',u'8',u'9',u'14',u'16',u'17',\n",
    "                          u'18',u'20',u'23',u'25',u'26',u'56',u'57',u'58'],axis=1))\n",
    "    \n",
    "    return df1, df2, df3, df4\n",
    "\n",
    "def encode(dataset):\n",
    "    drop_cols = []\n",
    "    for i in dataset.columns:\n",
    "        if isinstance(dataset[i].values[0], basestring):\n",
    "            drop_cols = drop_cols + [i]\n",
    "    \n",
    "    return encode_onehot(dataset, drop_cols)\n",
    "\n",
    "def encode_with_test(train, test):\n",
    "    agg_data = pd.concat([train, test],axis=0,ignore_index=True)\n",
    "    agg_data= encode(agg_data)\n",
    "    \n",
    "    return agg_data._slice(slice(0,train.shape[0]),axis=0), agg_data._slice(slice(train.shape[0],agg_data.shape[0]),axis=0)\n",
    "\n",
    "train = pd.read_csv('data.csv')\n",
    "label = train['label']\n",
    "train = train.drop('label',1)\n",
    "print len(train.columns)\n",
    "X_quiz = pd.read_csv('quiz.csv')\n",
    "\n",
    "#split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, label)\n",
    "train_1, train_2, train_3, train_4 = prepare_data(X_train)\n",
    "test_1, test_2, test_3, test_4 = prepare_data(X_test)\n",
    "quiz_1, quiz_2, quiz_3, quiz_4 = prepare_data(X_quiz)\n",
    "                             \n",
    "print train_4.shape\n",
    "    \n",
    "#agg_data = pd.concat([train, X_quiz],axis=0,ignore_index=True)\n",
    "\n",
    "\n",
    "#display(agg_data.shape[0])\n",
    "#train = agg_data._slice(slice(0,train.shape[0]),0)\n",
    "#X_quiz = agg_data._slice(slice(train.shape[0],agg_data.shape[0]),0)\n",
    "#pca = PCA(n_components=100)\n",
    "#train = pca.fit(train).transform(train)\n",
    "#display(train.shape)\n",
    "#display(X_quiz.shape)\n",
    "\n",
    "print len(train_1.columns) + len(train_2.columns) + len(train_3.columns)\n",
    "print train.shape, train_1.shape, train_2.shape, train_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train and test sets for blending.\n",
      "0 XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "1 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "2 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "3 ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "4 ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "Blending.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1     0.9299    0.9630    0.9462     13259\n",
      "          1     0.9512    0.9086    0.9294     10523\n",
      "\n",
      "avg / total     0.9393    0.9389    0.9387     23782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import classifier\n",
    "reload(classifier)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "\n",
    "train_1 = encode(X_train)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(train_1, y_train)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from classifier import blend_clf\n",
    "\n",
    "train_1 = encode(X_train)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(train_1, y_train)\n",
    "\n",
    "#gbc = GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10)\n",
    "clfs = [xgb.XGBClassifier(n_estimators=100,learning_rate=0.1,max_depth=7),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini', max_features='sqrt'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy', max_features='sqrt')]\n",
    "\n",
    "\n",
    "predictions = blend_clf(clfs, X_train1.as_matrix(),X_test1.as_matrix(),y_train1.as_matrix())\n",
    "print classification_report(y_test1, predictions,digits=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clf__average', 'clf__l1_ratio', 'clf__penalty', 'clf__warm_start', 'clf__random_state', 'clf', 'clf__n_iter', 'clf__learning_rate', 'clf__verbose', 'steps', 'clf__loss', 'clf__class_weight', 'clf__fit_intercept', 'clf__power_t', 'clf__n_jobs', 'clf__eta0', 'clf__alpha', 'clf__shuffle', 'clf__epsilon']\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "-- Epoch 1\n",
      "Norm: 23.02, NNZs: 33, Bias: 0.304084, T: 499, Avg. loss: 533.381897\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.81, NNZs: 33, Bias: -3.719816, T: 998, Avg. loss: 377.859071\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.63, NNZs: 33, Bias: -2.187557, T: 1497, Avg. loss: 299.342091\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.12, NNZs: 33, Bias: -0.819241, T: 1996, Avg. loss: 250.458674\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.35, NNZs: 33, Bias: -0.846817, T: 2495, Avg. loss: 216.775159\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 26.88, NNZs: 33, Bias: 0.846839, T: 500, Avg. loss: 540.865625\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.97, NNZs: 33, Bias: 0.740469, T: 1000, Avg. loss: 387.385494\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 24.69, NNZs: 33, Bias: 2.792540, T: 1500, Avg. loss: 308.386019\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 16.97, NNZs: 33, Bias: 1.808922, T: 2000, Avg. loss: 259.346941\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11.98, NNZs: 33, Bias: 1.339112, T: 2500, Avg. loss: 225.161174\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 22.89, NNZs: 31, Bias: -0.070369, T: 501, Avg. loss: 487.782520\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.67, NNZs: 31, Bias: 1.508519, T: 1002, Avg. loss: 351.323875\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.46, NNZs: 31, Bias: 1.318522, T: 1503, Avg. loss: 278.816314\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11.39, NNZs: 31, Bias: 0.860544, T: 2004, Avg. loss: 233.921081\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.35, NNZs: 31, Bias: 0.436168, T: 2505, Avg. loss: 202.772871\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.90, NNZs: 33, Bias: -0.028250, T: 499, Avg. loss: 116.536964\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.35, NNZs: 33, Bias: -0.092701, T: 998, Avg. loss: 71.175266\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.34, NNZs: 33, Bias: -0.134980, T: 1497, Avg. loss: 52.676746\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.10, NNZs: 33, Bias: -0.070841, T: 1996, Avg. loss: 42.327824\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.95, NNZs: 33, Bias: -0.038038, T: 2495, Avg. loss: 35.607442\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.63, NNZs: 33, Bias: -0.138793, T: 500, Avg. loss: 110.893957\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.08, NNZs: 33, Bias: -0.212900, T: 1000, Avg. loss: 69.600368\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.32, NNZs: 33, Bias: -0.097611, T: 1500, Avg. loss: 51.962776\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.26, NNZs: 33, Bias: -0.230210, T: 2000, Avg. loss: 41.971905\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.76, NNZs: 33, Bias: -0.152252, T: 2500, Avg. loss: 35.453884\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.06, NNZs: 31, Bias: 0.476496, T: 501, Avg. loss: 116.636635\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.19, NNZs: 31, Bias: 0.587823, T: 1002, Avg. loss: 70.546391\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.52, NNZs: 31, Bias: 0.448479, T: 1503, Avg. loss: 52.045175\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.13, NNZs: 31, Bias: 0.487663, T: 2004, Avg. loss: 41.722391\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.42, NNZs: 31, Bias: 0.492790, T: 2505, Avg. loss: 35.032694\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 16.44, NNZs: 32, Bias: 5.011831, T: 750, Avg. loss: 451.652656\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.59, NNZs: 32, Bias: 2.301155, T: 1500, Avg. loss: 304.544004\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.25, NNZs: 33, Bias: 3.000351, T: 2250, Avg. loss: 235.613425\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.37, NNZs: 33, Bias: 1.871957, T: 3000, Avg. loss: 194.494798\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11.81, NNZs: 33, Bias: 1.856283, T: 3750, Avg. loss: 166.831512\n",
      "Total training time: 0.00 seconds.\n",
      "Best score: 0.403\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.25      0.01      0.01       143\n",
      "          1       0.42      0.97      0.59       107\n",
      "\n",
      "avg / total       0.32      0.42      0.26       250\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "import classifier\n",
    "reload(classifier)\n",
    "\n",
    "from classifier import train_SGD_SVM\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svm_clf = train_SGD_SVM(X_train3, y_train3)\n",
    "predictions = svm_clf.predict(X_test3)\n",
    "print classification_report(y_test3, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.681\n",
      "Best parameters set:\n",
      "\tclf__n_estimators: 100\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.69      0.80      0.74       133\n",
      "          1       0.72      0.60      0.65       117\n",
      "\n",
      "avg / total       0.71      0.70      0.70       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import classifier\n",
    "reload(classifier)\n",
    "\n",
    "from classifier import train_ada_boost\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dt_clf = train_ada_boost(X_train3,y_train3)\n",
    "predictions = dt_clf.predict(X_test3)\n",
    "print classification_report(y_test3, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1     0.9254    0.9680    0.9462     13349\n",
      "          1     0.9565    0.9001    0.9275     10433\n",
      "\n",
      "avg / total     0.9390    0.9382    0.9380     23782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import classifier\n",
    "reload(classifier)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from classifier import train_xtree_classifer, train_rf, train_SGD_SVM, train_friedman_xgbm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# XTree is okay in set 3\n",
    "\n",
    "train_3 = encode(X_train)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(train_3, y_train)\n",
    "\n",
    "xtree = ExtraTreesClassifier(n_jobs=-1,n_estimators=200,max_depth=None, max_features='sqrt',random_state=0)\n",
    "xtree.fit(X_train3, y_train3)\n",
    "predictions = xtree.predict(X_test3)\n",
    "print classification_report(y_test3, predictions, digits=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
