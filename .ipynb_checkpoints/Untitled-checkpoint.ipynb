{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "(37500, 52)\n",
      "52\n",
      "(50000, 52) (37500, 10) (37500, 6) (37500, 36)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import classifier\n",
    "reload(classifier)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from classifier import encode_onehot\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def prepare_data(df):\n",
    "    df4 = df\n",
    "    df1 = pd.DataFrame(df.loc[:,[u'0',u'5',u'7',u'8',u'9',u'14',u'16',u'17',u'56',u'57']])\n",
    "    #0 , 26 8, 5 , 14, 23 , 58 -> boss\n",
    "    #17, 56, 16,9,, 25, 57 -> yes\n",
    "    #20, 18, 7 -> no\n",
    "    #df2 = pd.DataFrame(df.loc[:,[u'18',u'20',u'23',u'25',u'26']])\n",
    "    df2 = pd.DataFrame(df.loc[:,[u'18',u'20',u'23',u'25',u'26',u'58']])\n",
    "    df3 = pd.DataFrame(df.drop([u'0',u'5',u'7',u'8',u'9',u'14',u'16',u'17',\n",
    "                          u'18',u'20',u'23',u'25',u'26',u'56',u'57',u'58'],axis=1))\n",
    "    \n",
    "    return df1, df2, df3, df4\n",
    "\n",
    "def encode(dataset):\n",
    "    drop_cols = []\n",
    "    for i in dataset.columns:\n",
    "        if isinstance(dataset[i].values[0], basestring):\n",
    "            drop_cols = drop_cols + [i]\n",
    "    \n",
    "    return encode_onehot(dataset, drop_cols)\n",
    "\n",
    "def encode_with_test(train, test):\n",
    "    agg_data = pd.concat([train, test],axis=0,ignore_index=True)\n",
    "    agg_data= encode(agg_data)\n",
    "    \n",
    "    return agg_data._slice(slice(0,train.shape[0]),axis=0), agg_data._slice(slice(train.shape[0],agg_data.shape[0]),axis=0)\n",
    "\n",
    "train = pd.DataFrame.sample(pd.read_csv('data.csv'),1000)\n",
    "label = train['label']\n",
    "train = train.drop('label',1)\n",
    "print len(train.columns)\n",
    "X_quiz = pd.read_csv('quiz.csv')\n",
    "\n",
    "#split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, label)\n",
    "train_1, train_2, train_3, train_4 = prepare_data(X_train)\n",
    "test_1, test_2, test_3, test_4 = prepare_data(X_test)\n",
    "quiz_1, quiz_2, quiz_3, quiz_4 = prepare_data(X_quiz)\n",
    "                             \n",
    "print train_4.shape\n",
    "    \n",
    "#agg_data = pd.concat([train, X_quiz],axis=0,ignore_index=True)\n",
    "\n",
    "\n",
    "#display(agg_data.shape[0])\n",
    "#train = agg_data._slice(slice(0,train.shape[0]),0)\n",
    "#X_quiz = agg_data._slice(slice(train.shape[0],agg_data.shape[0]),0)\n",
    "#pca = PCA(n_components=100)\n",
    "#train = pca.fit(train).transform(train)\n",
    "#display(train.shape)\n",
    "#display(X_quiz.shape)\n",
    "\n",
    "print len(train_1.columns) + len(train_2.columns) + len(train_3.columns)\n",
    "print train.shape, train_1.shape, train_2.shape, train_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train and test sets for blending.\n",
      "0 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "1 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "2 ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "3 ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "4 GradientBoostingClassifier(init=None, learning_rate=0.3, loss='deviance',\n",
      "              max_depth=6, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "              presort='auto', random_state=None, subsample=0.5, verbose=0,\n",
      "              warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Blending.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1     0.9259    0.9533    0.9394      5309\n",
      "          1     0.9366    0.9004    0.9181      4066\n",
      "\n",
      "avg / total     0.9305    0.9303    0.9302      9375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import classifier\n",
    "reload(classifier)\n",
    "\n",
    "from classifier import blend_clf\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "train_1 = encode(X_train)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(train_1, y_train)\n",
    "\n",
    "\n",
    "#gbc = GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10)\n",
    "clfs = [RandomForestClassifier(n_estimators=50, n_jobs=-1, criterion='gini', random_state=1234),\n",
    "        RandomForestClassifier(n_estimators=50, n_jobs=-1, criterion='entropy', random_state=345),\n",
    "        ExtraTreesClassifier(n_estimators=50, n_jobs=-1, criterion='gini', max_features='sqrt', random_state=312),\n",
    "        ExtraTreesClassifier(n_estimators=50, n_jobs=-1, criterion='entropy', max_features='sqrt', random_sate=4567),\n",
    "        GradientBoostingClassifier(learning_rate=0.3, subsample=0.5, max_depth=6, n_estimators=50),\n",
    "        BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5,n_jobs=-1,random_state=851),\n",
    "        AdaBoostClassifier(n_estimators=50,learning_rate=0.3)]\n",
    "\n",
    "\n",
    "predictions = blend_clf(clfs, X_train1.as_matrix(),X_test1.as_matrix(),y_train1.as_matrix())\n",
    "print classification_report(y_test1, predictions,digits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 5227 features per sample; expecting 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-7f1914a7dc8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdigits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/miniconda2/lib/python2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \"\"\"\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/miniconda2/lib/python2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 249\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 5227 features per sample; expecting 4"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(X_test1.as_matrix())\n",
    "print classification_report(y_test1, predictions,digits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 4027)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1     0.9152    0.9592    0.9366      5241\n",
      "          1     0.9449    0.8873    0.9152      4134\n",
      "\n",
      "avg / total     0.9283    0.9275    0.9272      9375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import classifier\n",
    "reload(classifier)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from classifier import train_xtree_classifer, train_rf, train_SGD_SVM, train_friedman_xgbm\n",
    "from sklearn.metrics import classification_report\n",
    "## XTREE is extremely effective to Set 1\n",
    "display(train_1.shape)\n",
    "train_1 = encode(X_train)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(train_1, y_train)\n",
    "xtree = ExtraTreesClassifier(n_jobs=-1,n_estimators=200,max_depth=None, max_features='sqrt',random_state=12)\n",
    "xtree.fit(X_train1, y_train1)\n",
    "predictions = xtree.predict(X_test1)\n",
    "print classification_report(y_test1, predictions,digits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-174659ce92e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mxtree\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'exponential'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mxtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/miniconda2/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[1;32m-> 1025\u001b[1;33m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/miniconda2/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1078\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1079\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1080\u001b[1;33m                                      X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/miniconda2/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    782\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m--> 784\u001b[1;33m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/miniconda2/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    348\u001b[0m                                            max_leaf_nodes)\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_1 = encode(X_train)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(train_1, y_train)\n",
    "\n",
    "xtree =GradientBoostingClassifier(loss='exponential',n_estimators=300, learning_rate=0.1, random_state=0, subsample=0.2)\n",
    "\n",
    "xtree.fit(X_train1, y_train1)\n",
    "predictions = xtree.predict(X_test1)\n",
    "print classification_report(y_test1, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.92      0.82      0.87      1077\n",
      "          1       0.79      0.91      0.84       798\n",
      "\n",
      "avg / total       0.86      0.85      0.86      1875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "train_1 = encode(X_train)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(train_1, y_train)\n",
    "mnb = BernoulliNB(alpha=0.55)\n",
    "\n",
    "mnb.fit(X_train1, y_train1)\n",
    "predictions = mnb.predict(X_test1)\n",
    "print classification_report(y_test1, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 283.75, NNZs: 1552, Bias: -325.804089, T: 5625, Avg. loss: 5116.524409\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 220.87, NNZs: 1723, Bias: -323.304354, T: 11250, Avg. loss: 2951.077713\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 199.60, NNZs: 1797, Bias: -318.536513, T: 16875, Avg. loss: 2127.927144\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 187.99, NNZs: 1829, Bias: -313.307615, T: 22500, Avg. loss: 1678.605825\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 177.47, NNZs: 1853, Bias: -309.229986, T: 28125, Avg. loss: 1391.691489\n",
      "Total training time: 0.12 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.74      0.96      0.84      1120\n",
      "          1       0.89      0.50      0.64       755\n",
      "\n",
      "avg / total       0.80      0.77      0.76      1875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = SGDClassifier(loss=\"hinge\", penalty=\"l2\",shuffle=True,alpha=0.001,epsilon=0.5, verbose=1)\n",
    "\n",
    "sgd.fit(X_train1,y_train1)\n",
    "predictions = sgd.predict(X_test1)\n",
    "print classification_report(y_test1, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 2521.11, NNZs: 1558, Bias: -1471.580604, T: 5625, Avg. loss: 26702.512751\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2190.95, NNZs: 1744, Bias: -1535.015508, T: 11250, Avg. loss: 17262.810969\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1989.69, NNZs: 1830, Bias: -1540.604594, T: 16875, Avg. loss: 13036.384057\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1860.46, NNZs: 1870, Bias: -1516.497241, T: 22500, Avg. loss: 10577.067978\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1751.77, NNZs: 1908, Bias: -1489.447623, T: 28125, Avg. loss: 8916.537499\n",
      "Total training time: 0.12 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.91      0.87      0.89      1120\n",
      "          1       0.82      0.87      0.84       755\n",
      "\n",
      "avg / total       0.87      0.87      0.87      1875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = BernoulliNB(alpha=0.55)\n",
    "xtree = ExtraTreesClassifier(n_jobs=-1,n_estimators=200,max_depth=None, max_features='sqrt',random_state=0)\n",
    "sgd = SGDClassifier(loss=\"log\", penalty=\"l2\",shuffle=True,alpha=0.0001,epsilon=0.1, verbose=1)\n",
    "\n",
    "votes = VotingClassifier(estimators=[('mnb', mnb), ('xtree', xtree),('sgd',sgd)] , voting='soft')\n",
    "votes.fit(X_train1,y_train1)\n",
    "predictions = votes.predict(X_test1)\n",
    "print classification_report(y_test1, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
